# grafana_admin_pass: {vault.yml}

monitoring_namespace: monitoring
monitoring_release_name: prometheus-stack

# aliases of "homelab"
monitoring_host_names:
  prometheus:
    - metrics
    - prometheus
  thanos:
    - thanos
  alertmanager:
    - alerts
    - alertmanager
  grafana:
    - grafana
    - monitoring

# remember to add all domain names to pfSense DNS as
# aliases of homelab.fourteeners.local: 192.168.0.221
# https://docs.ansible.com/ansible/latest/playbook_guide/playbooks_filters.html#products
monitoring_fqdns: |
  {% set fqdns = {} %}
  {% for service, hosts in monitoring_host_names.items() %}
  {%   set _ = fqdns.update({
         service: hosts | product(search_domains) | map('join','.')
       }) %}
  {% endfor %}
  {{ fqdns  }}

monitoring_secrets:
  prometheus: monitoring-prometheus-tls
  thanos-tls: monitoring-thanos-tls
  thanos-config: monitoring-thanos-config
  alertmanager: monitoring-alertmanager-tls
  grafana: monitoring-grafana-tls
  scraper: monitoring-scraper-tls
  etcd: monitoring-etcd-tls
  oauth-proxy: monitoring-oauth-proxy
  credentials: monitoring-credentials

# https://thanos.io/tip/operating/https.md/
thanos_config_secret_data:
  http-config.yaml: |
    tls_server_config:
      cert_file: /tls/thanos/tls.crt
      key_file: /tls/thanos/tls.key
      client_ca_file: /tls/thanos/ca.crt
      client_auth_type: RequestClientCert

monitoring_oidc_client_id:
  prometheus: prometheus
  alertmanager: alertmanager
# monitoring_oidc_client_secret: {vault.yml}
# sso.fourteeners.local is same host as keycloak.fourteeners.local
monitoring_oidc_issuer_uri: https://sso.fourteeners.local/realms/homelab

# https://github.com/oauth2-proxy/oauth2-proxy/releases
monitoring_oauth2_proxy_version: "7.9.0"
# https://oauth2-proxy.github.io/oauth2-proxy/configuration/overview/#general-provider-options
# https://oauth2-proxy.github.io/oauth2-proxy/configuration/providers/keycloak_oidc
# (references "redis_cluster_urls" var from vars/valkey.yml)
monitoring_oauth2_proxy_config: |
  {% set configs = {} %}
  {% for app in [
         'prometheus',
         'alertmanager'
       ] %}
  {%   set config = {
         'provider':              'keycloak-oidc',
         'provider_display_name': 'Keycloak',
         'skip_provider_button':   true,
         'provider_ca_files':    ['/tls/server/ca.crt'],
         'client_id':        monitoring_oidc_client_id[app],
         'client_secret':    monitoring_oidc_client_secret[app],
         'code_challenge_method': 'S256',
         'redirect_url':    'https://'~ monitoring_fqdns[app][0] ~'/oauth2/callback',
         'oidc_issuer_url':  monitoring_oidc_issuer_uri,
         'cookie_secret':    oauth2_proxy_cookie_secret,
         'cookie_domains':  '.'~ search_domains[0],
         'cookie_samesite': 'lax',
         'email_domains':   ['*'],
                 'tls_cert_file': '/tls/server/tls.crt',
                 'tls_key_file':  '/tls/server/tls.key',
         'metrics_tls_cert_file': '/tls/server/tls.crt',
         'metrics_tls_key_file':  '/tls/server/tls.key',
         'session_store_type':            'redis',
         'redis_use_cluster':              true,
         'redis_insecure_skip_tls_verify': true,
         'redis_cluster_connection_urls':  redis_cluster_urls,
         'redis_password':                 valkey_pass
       } %}
  {#   to_toml: filter_plugins/filters.py #}
  {%   set _ = configs.update({
         app ~'.toml': config | to_toml
       }) %}
  {% endfor  %}
  {{ configs }}

metric_scraper_tls_config:
  cert: # SecretOrConfigMap
    secret:
      name: "{{ monitoring_secrets['scraper'] }}"
      key: tls.crt
  keySecret: # SecretKeySelector
    name: "{{ monitoring_secrets['scraper'] }}"
    key: tls.key
  ca: # SecretOrConfigMap
    secret:
      name: "{{ monitoring_secrets['scraper'] }}"
      key: ca.crt
  insecureSkipVerify: true

prometheus_stack_chart_version: "70.8.0"
# https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack
# https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack/values.yaml
# https://github.com/prometheus-operator/prometheus-operator/tree/main/Documentation/api-reference/api.md
prometheus_stack_chart_values:
  # NOTE: if changing the stack/release name and re-installing the Helm release,
  # be sure to manually delete the "<stack_name>-kubelet" service in kube-system
  # namespace, or else "found duplicate series for the match group" error occurs
  nameOverride: "{{ monitoring_release_name }}"

  # https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack/charts/crds
  crds:
    enabled: true
    upgradeJob: # since Helm has no ability
      enabled: true # feature is in preview

  # ======================================== Operator ========================================
  #
  # https://prometheus-operator.dev/docs/
  # https://prometheus-operator.dev/docs/getting-started/installation/#install-using-helm-chart
  prometheusOperator:
    enabled: true
    tls:
      enabled: true
    revisionHistoryLimit: 1

    prometheusInstanceNamespaces:
      - "{{ monitoring_namespace }}"
    thanosRulerInstanceNamespaces:
      - "{{ monitoring_namespace }}"
    alertmanagerInstanceNamespaces:
      - "{{ monitoring_namespace }}"
    alertmanagerConfigNamespaces: []

    resources:
      requests:
        cpu: 10m
        memory: 24Mi
      limits:
        cpu: 50m
        memory: 64Mi

    serviceMonitor:
      selfMonitor: true

  # ======================================= Prometheus =======================================
  #
  # https://prometheus.io/docs/prometheus/latest/configuration/configuration/
  prometheus:
    enabled: true
    agentMode: false

    # https://github.com/prometheus-operator/prometheus-operator/tree/main/Documentation/api-reference/api.md#prometheusspec
    prometheusSpec:
      # total pods = replicas * shards
      replicas: 3
      shards: 1

      # soft:  prefer different nodes
      # hard: require different nodes
      podAntiAffinity: hard

      resources:
        requests:
          cpu: 250m
          memory: 512Mi

      # https://github.com/prometheus-operator/prometheus-operator/tree/main/Documentation/platform/storage.md
      storageSpec:
        volumeClaimTemplate:
          spec:
            storageClassName: "{{ default_storage_class }}"
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 3Gi

      persistentVolumeClaimRetentionPolicy:
        whenScaled: Retain
        whenDeleted: Delete

      # data retention period
      retention: 12h

      tsdb:
        # allow out-of-order samples because
        # there may be lag in a slow cluster
        outOfOrderTimeWindow: 5s

      # https://github.com/prometheus-operator/prometheus-operator/tree/main/Documentation/api-reference/api.md#queryspec
      query: # QuerySpec
        maxConcurrency: 4
        maxSamples: 50000
        timeout: 30s

      externalUrl: https://{{ monitoring_fqdns['prometheus'] | first }}

      # https://github.com/prometheus-operator/prometheus-operator/tree/main/Documentation/api-reference/api.md#prometheuswebspec
      web: # PrometheusWebSpec
        # WebTLSConfig (web server certificate):
        # https://github.com/prometheus-operator/prometheus-operator/tree/main/Documentation/api-reference/api.md#webtlsconfig
        tlsConfig:
          # IMPORTANT! file-based configs using certFile/keyFile/clientCAFile
          # do NOT work--they yield an invalid pod volume SecretVolumeSource
          # for "web-config-tls-secret-key-*" that has no "secretName" field

          cert: # SecretOrConfigMap
            secret: # SecretKeySelector
              name: "{{ monitoring_secrets['prometheus'] }}"
              key: tls.crt
          # certFile: /tls/server/tls.crt
          keySecret: # SecretKeySelector
            name: "{{ monitoring_secrets['prometheus'] }}"
            key: tls.key
          # keyFile: /tls/server/tls.key
          client_ca: # SecretOrConfigMap
            secret:
              name: "{{ monitoring_secrets['prometheus'] }}"
              key: ca.crt
          # clientCAFile: /tls/server/ca.crt
          # https://pkg.go.dev/crypto/tls#ClientAuthType
          clientAuthType: RequestClientCert

      volumes:
        - name: tls-server
          secret:
            secretName: "{{ monitoring_secrets['prometheus'] }}"
        - name: tls-scraper
          secret:
            secretName: "{{ monitoring_secrets['scraper'] }}"
        - name: tls-etcd # scraper
          secret:
            secretName: "{{ monitoring_secrets['etcd'] }}"
        # provide Thanos certs for gRPC server and
        # for verifying MinIO's server certificate
        - name: tls-thanos
          secret:
            secretName: "{{ monitoring_secrets['thanos-tls'] }}"
        # provide Thanos file for --http.config
        - name: thanos-config
          secret:
            secretName: "{{ monitoring_secrets['thanos-config'] }}"
        - name: oauth-proxy
          secret:
            secretName: "{{ monitoring_secrets['oauth-proxy'] }}"

      # mounts for all containers except extras
      # like "thanos-sidecar" and "oauth-proxy"
      volumeMounts:
        - name: tls-server
          mountPath: /tls/server
          readOnly: true
        - name: tls-scraper
          mountPath: /tls/scraper
          readOnly: true
        - name: tls-etcd
          mountPath: /tls/etcd
          readOnly: true

      # inject additional containers, e.g. adding
      # an authentication proxy like oauth2-proxy
      containers:
        # this performs a merge patch on the
        # operator-generated "thanos-sidecar"
        - name: thanos-sidecar
          volumeMounts:
            - name: tls-thanos
              mountPath: /tls/thanos
              readOnly: true
            - name: thanos-config
              # /etc/thanos/config is already mounted
              # with prometheus.http-client-file.yaml
              mountPath: /etc/thanos/config2
              readOnly: true

        # https://github.com/oauth2-proxy/oauth2-proxy
        - name: oauth-proxy
          image: quay.io/oauth2-proxy/oauth2-proxy:v{{ monitoring_oauth2_proxy_version }}

          args:
            # https://oauth2-proxy.github.io/oauth2-proxy/configuration/overview#server-options
            # see .service.additionalPorts
            - --https-address=0.0.0.0:10090
            - --metrics-secure-address=0.0.0.0:10091
            # requires IP SAN in server cert
            - --upstream=https://127.0.0.1:9090
            # see monitoring_oauth2_proxy_config
            - --config=/etc/oauth2-proxy.cfg

          volumeMounts:
            - name: tls-server
              mountPath: /tls/server
              readOnly: true
            - name: oauth-proxy
              mountPath: /etc/oauth2-proxy.cfg
              subPath: prometheus.toml
              readOnly: true

          ports:
            - name: oauth-proxy
              containerPort: 10090
              protocol: TCP
            - name: oauth-metrics
              containerPort: 10091
              protocol: TCP

      # https://github.com/prometheus-operator/prometheus-operator/tree/main/Documentation/api-reference/api.md#thanosspec
      thanos: # ThanosSpec
        objectStorageConfig:
          # create secret containing MinIO credentials:
          # https://thanos.io/tip/thanos/storage.md/#s3
          secret:
            type: S3
            config:
              # both bucket and user are
              # created by vars/minio.yml
              bucket: metrics
              endpoint: "{{ minio_s3api_fqdn }}"
              region: us-east-1
              access_key: thanos
              secret_key: "{{ minio_client_pass }}"
              http_config:
                tls_config:
                  # mounted by patched container
                  cert_file: /tls/thanos/tls.crt
                  key_file: /tls/thanos/tls.key
                  ca_file: /tls/thanos/ca.crt

        grpcServerTlsConfig:
          # https://github.com/prometheus-operator/prometheus-operator/tree/main/Documentation/api-reference/api.md#tlsconfig
          # only certFile, keyFile, caFile are supported
          certFile: /tls/thanos/tls.crt
          keyFile: /tls/thanos/tls.key
          caFile: /tls/thanos/ca.crt
          insecureSkipVerify: true

        # https://thanos.io/tip/components/sidecar.md/
        additionalArgs:
          # https://thanos.io/tip/operating/https.md/
          - name: http.config
            value: /etc/thanos/config2/http-config.yaml

      # https://github.com/prometheus-operator/prometheus-operator/tree/main/Documentation/api-reference/api.md#alertmanagerendpoints
      alertingEndpoints: # AlertmanagerEndpoints[]
        - name: "{{ monitoring_release_name }}-alertmanager"
          port: http-web
          scheme: https
          enableHttp2: true
          pathPrefix: /
          tlsConfig:
            certFile: /tls/server/tls.crt
            keyFile: /tls/server/tls.key
            caFile: /tls/server/ca.crt
            # must skip certificate verification here because
            # the Prometheus notifier component fails to send
            # alerts to Alertmanager due to failure to verify
            # the certificate because it does not contain any
            # IP SANs: https://10.42.xx.xx:9093/api/v2/alerts
            # and IP SAN cannot contain wildcards for pod IPs
            insecureSkipVerify: true

    # ==END== prometheusSpec

    service:
      enabled: true
      port: 9090
      targetPort: http-web

      additionalPorts:
        - name: oauth-proxy
          port: 10090 # ingress
          targetPort: oauth-proxy
        - name: oauth-metrics
          port: 10091
          targetPort: oauth-metrics

    ingress:
      enabled: true
      tls:
        - secretName: "{{ monitoring_secrets['prometheus'] }}"
          hosts: "{{ monitoring_fqdns['prometheus'] }}"
      ingressClassName: "{{ rke_ingress_class }}"
      annotations:
        nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
        nginx.ingress.kubernetes.io/ssl-passthrough: "true"
      hosts: "{{ monitoring_fqdns['prometheus'] }}"
      paths: ["/"]
      pathType: Prefix
      servicePort: 10090 # oauth-proxy

    serviceMonitor:
      selfMonitor: true
      scheme: https

      # TLSConfig (mTLS client certificate):
      # https://github.com/prometheus-operator/prometheus-operator/tree/main/Documentation/api-reference/api.md#tlsconfig
      tlsConfig: "{{ metric_scraper_tls_config }}"

    thanosService: # for Thanos service discovery
      enabled: true

    # https://thanos.io/tip/components/query.md
    thanosServiceExternal: # for Thanos ingress
      enabled: true
      type: ClusterIP

    thanosIngress:
      enabled: true
      tls:
        - secretName: "{{ monitoring_secrets['thanos-tls'] }}"
          hosts: "{{ monitoring_fqdns['thanos'] }}"
      ingressClassName: "{{ rke_ingress_class }}"
      annotations:
        nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
      hosts: "{{ monitoring_fqdns['thanos'] }}"
      paths: ["/"]
      pathType: Prefix

    thanosServiceMonitor:
      enabled: true
      scheme: https
      tlsConfig: "{{ metric_scraper_tls_config }}"

    # for oauth-proxy sidecars
    additionalServiceMonitors: |
      {% set smons = [] %}
      {% for app in [
             'prometheus',
             'alertmanager'
           ] %}
      {%   set svc = monitoring_release_name ~'-'~ app %}
      {%   set _   = smons.append({
             'name':  svc ~'-oauth-proxy',
             'additionalLabels': {
               'app': svc
             },
             'selector': {
               'matchLabels': {
                 'app':           svc,
                 'release':       monitoring_release_name,
                 'self-monitor': 'true'
               }
             },
             'namespaceSelector': {
               'matchNames': [monitoring_namespace]
             },
             'endpoints': [{
               'port':       'oauth-metrics',
               'path':       '/metrics',
               'scheme':     'https',
               'tlsConfig':   metric_scraper_tls_config,
               'enableHttp2': true
             }]
           }) %}
      {% endfor %}
      {{ smons  }}

  # best to keep rules on Prometheus itself:
  # https://thanos.io/tip/components/rule.md
  thanosRuler:
    enabled: false

  # ====================================== Alertmanager ======================================
  #
  # https://prometheus.io/docs/alerting/alertmanager/
  alertmanager:
    enabled: true

    # https://github.com/prometheus-operator/prometheus-operator/tree/main/Documentation/api-reference/api.md#alertmanagerspec
    alertmanagerSpec:
      replicas: 3

      # soft:  prefer different nodes
      # hard: require different nodes
      podAntiAffinity: hard

      resources:
        requests:
          cpu: 50m
          memory: 64Mi

      # https://github.com/prometheus-operator/prometheus-operator/tree/main/Documentation/platform/storage.md
      storage:
        volumeClaimTemplate:
          spec:
            storageClassName: "{{ default_storage_class }}"
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 512Mi

      persistentVolumeClaimRetentionPolicy:
        whenScaled: Retain
        whenDeleted: Delete

      # data retention period
      retention: 168h # 7 days

      externalUrl: https://{{ monitoring_fqdns['alertmanager'] | first }}
      scheme: https

      # https://github.com/prometheus-operator/prometheus-operator/tree/main/Documentation/api-reference/api.md#alertmanagerwebspec
      web: # AlertmanagerWebSpec
        # WebTLSConfig (web server certificate):
        # https://github.com/prometheus-operator/prometheus-operator/tree/main/Documentation/api-reference/api.md#webtlsconfig
        tlsConfig: &alerts_tls
          cert: # SecretOrConfigMap
            secret: # SecretKeySelector
              name: "{{ monitoring_secrets['alertmanager'] }}"
              key: tls.crt
          keySecret: # SecretKeySelector
            name: "{{ monitoring_secrets['alertmanager'] }}"
            key: tls.key
          client_ca: # SecretOrConfigMap
            secret:
              name: "{{ monitoring_secrets['alertmanager'] }}"
              key: ca.crt
          # https://pkg.go.dev/crypto/tls#ClientAuthType
          clientAuthType: RequestClientCert

      # TLSConfig (mTLS client certificate):
      # https://github.com/prometheus-operator/prometheus-operator/tree/main/Documentation/api-reference/api.md#tlsconfig
      tlsConfig: *alerts_tls

      volumes:
        - name: tls-server
          secret:
            secretName: "{{ monitoring_secrets['alertmanager'] }}"
        - name: oauth-proxy
          secret:
            secretName: "{{ monitoring_secrets['oauth-proxy'] }}"

      # inject additional containers, e.g. adding
      # an authentication proxy like oauth2-proxy
      containers:
        # https://github.com/oauth2-proxy/oauth2-proxy
        - name: oauth-proxy
          image: quay.io/oauth2-proxy/oauth2-proxy:v{{ monitoring_oauth2_proxy_version }}

          args:
            # https://oauth2-proxy.github.io/oauth2-proxy/configuration/overview#server-options
            # see .service.additionalPorts
            - --https-address=0.0.0.0:10093
            - --metrics-secure-address=0.0.0.0:10094
            # requires IP SAN in server cert
            - --upstream=https://127.0.0.1:9093
            # see monitoring_oauth2_proxy_config
            - --config=/etc/oauth2-proxy.cfg

          volumeMounts:
            - name: tls-server
              mountPath: /tls/server
              readOnly: true
            - name: oauth-proxy
              mountPath: /etc/oauth2-proxy.cfg
              subPath: alertmanager.toml
              readOnly: true

          ports:
            - name: oauth-proxy
              containerPort: 10093
              protocol: TCP
            - name: oauth-metrics
              containerPort: 10094
              protocol: TCP

    # ==END== alertmanagerSpec

    service:
      enabled: true
      port: 9093
      targetPort: http-web

      additionalPorts:
        - name: oauth-proxy
          port: 10093 # ingress
          targetPort: oauth-proxy
        - name: oauth-metrics
          port: 10094
          targetPort: oauth-metrics

    ingress:
      enabled: true
      tls:
        - secretName: "{{ monitoring_secrets['alertmanager'] }}"
          hosts: "{{ monitoring_fqdns['alertmanager'] }}"
      ingressClassName: "{{ rke_ingress_class }}"
      annotations:
        nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
        nginx.ingress.kubernetes.io/ssl-passthrough: "true"
      hosts: "{{ monitoring_fqdns['alertmanager'] }}"
      paths: ["/"]
      pathType: Prefix
      servicePort: 10093 # oauth-proxy

    serviceMonitor:
      selfMonitor: true
      scheme: https
      tlsConfig: "{{ metric_scraper_tls_config }}"

  # ======================================== Grafana ========================================
  #
  # https://prometheus.io/docs/visualization/grafana/
  grafana: # subchart values
    enabled: true

    defaultDashboardsEnabled: true
    defaultDashboardsEditable: true
    defaultDashboardsTimezone: browser
    defaultDashboardsInterval: 5m

    # settings below are defined by Grafana Helm chart:
    # https://github.com/grafana/helm-charts/tree/main/charts/grafana
    # https://github.com/grafana/helm-charts/tree/main/charts/grafana/values.yaml
    replicas: 1
    revisionHistoryLimit: 1

    admin:
      existingSecret: "{{ monitoring_secrets['credentials'] }}"
      userKey: grafana-admin-username
      passwordKey: grafana-admin-password

    # https://docs.grafana.org/installation/configuration/
    grafana.ini:
      # maps at this level correspond to INI file sections:
      # https://github.com/grafana/helm-charts/tree/main/charts/grafana/templates/_config.tpl
      server:
        # https://grafana.com/docs/grafana/latest/setup-grafana/set-up-https/#configure-grafana-https-and-restart-grafana
        # https://grafana.com/docs/grafana/latest/setup-grafana/configure-grafana/#server
        protocol: https
        cert_file: /tls/server/tls.crt
        cert_key: /tls/server/tls.key

    readinessProbe:
      httpGet:
        scheme: HTTPS
    livenessProbe:
      httpGet:
        scheme: HTTPS

    persistence:
      enabled: true
      type: StatefulSet
      storageClassName: "{{ default_storage_class }}"
      accessModes: ["ReadWriteOnce"]
      size: 2Gi
      # allow PVC deletion when
      # Helm release is deleted
      finalizers: []

    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        cpu: 500m
        memory: 640Mi

    # https://github.com/kiwigrid/k8s-sidecar
    sidecar:
      dashboards:
        enabled: true
        env: &sidecar_env
          REQ_SKIP_TLS_VERIFY: "true"
          PYTHONWARNINGS: ignore:Unverified HTTPS request
        reloadURL: https://localhost:3000/api/admin/provisioning/dashboards/reload
        enableNewTablePanelSyntax: true
      datasources:
        enabled: true
        env: *sidecar_env
        reloadURL: https://localhost:3000/api/admin/provisioning/datasources/reload
        # disable default (Prometheus) data source because its endpoint URL in
        # the Helm template does not use HTTPS, and there is no way to specify
        # jsonData.tlsAuthWithCACert and secureJsonData.tlsCACert using values
        # in this block. Instead, we use the additionalDataSources block below
        defaultDatasourceEnabled: false
        alertmanager:
          # due to the same issue as above, we
          # define using additionalDataSources
          enabled: false
      plugins:
        enabled: false
      alerts:
        enabled: false
      notifiers:
        enabled: false

    # https://grafana.com/docs/grafana/latest/administration/provisioning/#datasources
    # https://boredconsultant.com/2022/06/27/Grafana-Datasource-With-Custom-CA-Certificate/
    # https://grafana.com/docs/grafana/latest/administration/provisioning/#example-data-source-configuration-file
    additionalDataSources:
      - name: Prometheus
        type: prometheus
        uid: prometheus
        url: https://{{ monitoring_release_name }}-prometheus.{{ monitoring_namespace }}.svc.cluster.local:9090/
        access: proxy
        jsonData:
          tlsAuthWithCACert: true
          timeInterval: 30s
        secureJsonData:
          tlsCACert: "{{ ca_certs }}"
        isDefault: true

      - name: Alertmanager
        type: alertmanager
        uid: alertmanager
        url: https://{{ monitoring_release_name }}-alertmanager.{{ monitoring_namespace }}.svc.cluster.local:9093/
        access: proxy
        jsonData:
          tlsAuthWithCACert: true
          implementation: prometheus
          handleGrafanaManagedAlerts: false
        secureJsonData:
          tlsCACert: "{{ ca_certs }}"

    extraSecretMounts:
      - name: tls-server
        secretName: "{{ monitoring_secrets['grafana'] }}"
        mountPath: /tls/server
        readOnly: true

    # inject additional containers, e.g. adding
    # an authentication proxy like oauth2-proxy
    extraContainers: ""

    service:
      port: 443
      targetPort: 3000

    ingress:
      enabled: true
      tls:
        - secretName: "{{ monitoring_secrets['grafana'] }}"
          hosts: "{{ monitoring_fqdns['grafana'] }}"
      ingressClassName: "{{ rke_ingress_class }}"
      annotations:
        nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
        nginx.ingress.kubernetes.io/ssl-passthrough: "true"

        # nginx.ingress.kubernetes.io/server-snippet: |
        #   if ($host !~* ^({{ monitoring_host_names['grafana'] | join('|') }})\.) {
        #     # send Misdirected Request response
        #     # (result of connection coalescing)
        #     return 421;
        #   }
      hosts: "{{ monitoring_fqdns['grafana'] }}"
      path: /
      pathType: Prefix

    serviceMonitor:
      enabled: true
      labels:
        # Prometheus Operator will not be able to find
        # this ServiceMonitor without these labels and
        # the Grafana Overview dashboard will be empty:
        # https://github.com/prometheus-community/helm-charts/issues/3800
        app: "{{ monitoring_release_name }}-grafana"
        release: "{{ monitoring_release_name }}"
      scheme: https
      tlsConfig: "{{ metric_scraper_tls_config }}"

  # =================================== kube-state-metrics ===================================
  #
  # https://github.com/kubernetes/kube-state-metrics
  kubeStateMetrics:
    enabled: true

  # https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-state-metrics
  # https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-state-metrics/values.yaml
  kube-state-metrics: # subchart values
    resources:
      requests:
        cpu: 10m
        memory: 16Mi
      limits:
        cpu: 50m
        memory: 64Mi

    volumes:
      # volume to be mounted in
      # kube-rbac-proxy sidecar
      - name: tls-proxy
        secret:
          secretName: "{{ monitoring_secrets['prometheus'] }}"

    # https://github.com/brancz/kube-rbac-proxy#usage
    kubeRBACProxy:
      # disabling for now due to bug in the Helm chart
      # with readiness and liveness probes using HTTPS:
      # https://github.com/prometheus-community/helm-charts/issues/5529
      enabled: false

      volumeMounts:
        - name: tls-proxy
          mountPath: /tls/proxy
          readOnly: true

      extraArgs:
        - --tls-cert-file=/tls/proxy/tls.crt
        - --tls-private-key-file=/tls/proxy/tls.key
        - --client-ca-file=/tls/proxy/ca.crt

    prometheus:
      monitor:
        enabled: true
        # disable HTTPS due to kubeRBACProxy.
        # values below apply to same settings
        # under .http and .metrics endpoints
        # scheme: https
        # tlsConfig: "{{ metric_scraper_tls_config }}"
        # enableHttp2: true

    collectors:
      - certificatesigningrequests
      - configmaps
      - cronjobs
      - daemonsets
      - deployments
      - endpoints
      - horizontalpodautoscalers
      - ingresses
      - jobs
      - leases
      - limitranges
      - mutatingwebhookconfigurations
      - namespaces
      - networkpolicies
      - nodes
      - persistentvolumeclaims
      - persistentvolumes
      - poddisruptionbudgets
      - pods
      - replicasets
      - replicationcontrollers
      - resourcequotas
      - secrets
      - services
      - statefulsets
      - storageclasses
      - validatingwebhookconfigurations
      - volumeattachments
      # - ingressclasses
      # - clusterrolebindings
      # - clusterroles
      # - roles

  # ====================================== Node Exporter =====================================
  #
  # https://github.com/prometheus/node_exporter
  nodeExporter: # deploy DaemonSet
    enabled: true

    operatingSystems:
      linux:
        enabled: true
      aix:
        enabled: false
      darwin:
        enabled: false

  # https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-node-exporter
  # https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-node-exporter/values.yaml
  prometheus-node-exporter: # subchart values
    resources:
      requests:
        cpu: 10m
        memory: 16Mi
      limits:
        cpu: 50m
        memory: 64Mi

    extraVolumes:
      - name: tls-proxy
        secret:
          secretName: "{{ monitoring_secrets['prometheus'] }}"

    # https://github.com/brancz/kube-rbac-proxy#usage
    kubeRBACProxy:
      enabled: true

      extraVolumeMounts:
        - name: tls-proxy
          mountPath: /tls/proxy
          readOnly: true

      extraArgs:
        # default args already specify --config-file, but mounted
        # ConfigMap will be overridden by ".extraManifests" below
        - --tls-cert-file=/tls/proxy/tls.crt
        - --tls-private-key-file=/tls/proxy/tls.key
        - --client-ca-file=/tls/proxy/ca.crt

    prometheus:
      monitor:
        enabled: true
        scheme: https
        tlsConfig: "{{ metric_scraper_tls_config }}"
        scrapeTimeout: 30s

  # ======================================== Exporters =======================================
  #
  kubelet:
    enabled: true

  # requires RKE2 server options "etcd-expose-metrics: true"
  # and "etcd-arg: - listen-metrics-urls=http://node-ip:2381"
  kubeEtcd:
    enabled: true
    service:
      enabled: true
      selector:
        component: etcd

    serviceMonitor:
      enabled: true
      scheme: http
      # must use file-based certs
      # certFile: /tls/etcd/tls.crt
      # keyFile: /tls/etcd/tls.key
      # caFile: /tls/etcd/ca.crt

  kubeApiServer:
    enabled: true

  # requires component arg
  # "bind-address=0.0.0.0"
  kubeControllerManager:
    enabled: true
    service:
      enabled: true
      selector:
        component: kube-controller-manager

  # requires component arg
  # "bind-address=0.0.0.0"
  kubeScheduler:
    enabled: true
    service:
      enabled: true
      selector:
        component: kube-scheduler

  kubeProxy:
    enabled: true
    service:
      enabled: true
      selector:
        component: kube-proxy

  coreDns: # no kubeDns
    enabled: true
    service:
      enabled: true
      selector:
        k8s-app: kube-dns

  # ======================================== Manifests =======================================
  #
  extraManifests:
    - apiVersion: v1
      kind: ConfigMap
      metadata:
        # this overrides the ConfigMap created by the prometheus-node-exporter
        # Helm chart because the provided resource-based authorization doesn't
        # work--maybe the ServiceAccount doesn't have ClusterRole that allows
        # "tokenreviews" & "subjectaccessreviews"? So we use static rule that
        # only allows the client cert holder to access the /metrics endpoint:
        # https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-node-exporter/templates/rbac-configmap.yaml
        # https://github.com/brancz/kube-rbac-proxy/tree/master/examples/static-auth
        name: "{{ monitoring_release_name }}-prometheus-node-exporter-rbac-config"
      data:
        config-file.yaml: |+
          authorization:
            static:
              - user: # cert CN
                  name: scraper
                verb: get
                resourceRequest: false
                path: /metrics
